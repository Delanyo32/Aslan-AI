# Aslan AI

## Summary

Aslan AI is a pipeline architecture project aimed at capturing, transforming, and analyzing data to generate and improve predictions using techniques such as wave reduction, bootstrapping, and traveling salesman optimization. The project involves creating probabilistic models, testing for accuracy, refining node connections, and implementing an API-driven worker architecture. Key features include data parsing, weighted random selection, iterative improvement through bootstrapping, and advanced algorithms like Ant Colony Optimization to refine solution quality.

---

## Table of Contents
- [Feature Components](#feature-components)
- [Steps](#steps)
- [Testing and Improving](#testing-and-improving)
- [Ant Colony Optimization](#ant-colony-optimization)
- [Architecture](#architecture)
- [Technologies Used](#technologies-used)
- [Configuration Variables](#configuration-variables)
- [TODO](#todo)
- [Done](#done)
- [Notes](#notes)
- [Useful Commands](#useful-commands)
- [Reference](#reference)

---

## Feature Components

- **Data Collection and Parsing**  
- **Wave Reduce**: Generate possible outcomes for testing.  
- **Bootstrapping**: Narrow down possibilities to identify the most likely options.  

---

## Steps

1. Use data to capture and transform information.  
2. Use Wave Reduce to generate possible outcomes.  
3. Apply bootstrapping to find the most likely options.  
4. Gleam detailed information about the most likely option.  
5. Evaluate decision accuracy.  
6. Use the Traveling Salesman algorithm to improve node connections.

---

## Testing and Improving

- Test and find best solutions generated by Wave Reduce (can yield very small standard deviation).  
- Check if multiple iterations of bootstrapping improve the standard deviation.  
- Test bootstrapping using the best standard deviation generated from Wave Reduce.  
- Analyze which standard deviations affect trend direction the most.  
- Evaluate simulated profits.  
- Integrate Wave Reduce and bootstrapping into a training algorithm to improve node neighbors.  
- Experiment with timing structures for data, using "partitions" as terminology.  
- Diagram data and function flow to assist API development.  
- Develop structure to test different code implementations.  
- Build API for application workflow streamlining.

---

## Ant Colony Optimization

- Create a matrix of vector data with weights.  
- Run a bootstrapping round and log the standard deviation.  
- Add the inverse of the standard deviation to weights of selected values during bootstrapping.  
- Repeat bootstrapping until the smallest standard deviation is achieved.  
- Over multiple iterations, the solution improves continuously based on updated weights.

---

## Architecture

**Pipeline Architecture:**

- Data fetching  
- Parsing  
- Bootstrapping  

---

## Technologies Used

- **Burn** — A machine learning library written in Rust for efficient and scalable ML workflows.  
- **Polars** — A fast DataFrame library in Rust, serving as a high-performance alternative to NumPy and Pandas for data manipulation.  
- **Actix-web** — A powerful, pragmatic, and extremely fast web framework for Rust used to build HTTP services and APIs.
- **Redis** - For data storage and caching
- **Datadog** - For logging and analalitics

---

## Configuration Variables

- Stock symbol  
- Date range  
- Normalization value  
- Time frame  

---

## TODO

- Use SurrealDB for node modeling and data storage.  
- Implement nearest neighbor for improved selection directions.  
- Add nearest node for node connection.  
- Implement probabilistic selection.  
- Increase number of bootstrapping iterations with testing.  
- Improve standard deviation.  
- Decouple from actual datetime for bar data.  
- Generate test data focusing on connecting actual to test data.  
- Redesign engine structure to make it more generic.  
- Refactor data to increase generality.  
- Switch to worker architecture.  
- Evaluate correctness and improve accuracy.  
- Propagate across clusters of possibilities to reduce solution space by removing mismatched solutions and weighting matches.  
- Run bootstrapping to generate possibilities and improve weights.  
- Decouple code components.  
- Increase data scope.  
- Load and unload node data dynamically.  
- Add metrics and logging.

---

## Done

- Split data.  
- Add bidirectional generation.  
- Weight segment connections.  
- Gather test data.  
- Create data chunks.  
- Convert values to relative values or distance magnitudes.  
- Remove duplicate nodes.  
- Initialize connection weights.

---

## Notes

- Load keys from environment files.  
- When mixing mutability and immutability, always copy immutable data before mutating.  
- Chunking groups similar and related data to recognize preliminary patterns.  
- Compare approach with random stock picking and trades.  
- Use Traveling Salesman Algorithm to improve predictions and approach optimal solutions.

---

## Useful Commands

```bash
docker pull delanyo32/task-scheduler:latest
docker pull delanyo32/aslan-core:latest
docker pull redis
docker pull datadog/agent

docker network create --driver bridge aslan-core-net

docker run -it -p 9000:9000 -d --net "aslan-core-net" delanyo32/aslan-core bash 

docker container exec -it <container_id> bash 

curl -w '\n' http://chart-redis-c816035e:6379/ping
curl -w '\n' http://chart-taskschedulerdeploymentservice-c80e97cf:8080/health
curl -w '\n' http://chart-aslancoreservice-c8f2509a:9000

curl -Is http://chart-redis-c816035e:6379 | head -1

docker run --name redis -p 6379:6379 -d --net "aslan-core-net" redis 
docker run -dp 9000:9000 delanyo32/aslan-core --network aslan-core-net
docker run -dp 8080:8080 delanyo32/task-scheduler
docker run -d --cgroupns host --pid host --name dd-agent \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  -v /proc/:/host/proc/:ro \
  -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \
  -e DD_API_KEY=<DATADOG_API_KEY> gcr.io/datadoghq/agent:7

helm install aslan-logging -f values.yaml --set datadog.apiKey=<DATADOG_API_KEY> datadog/datadog --set targetSystem=linux

kubectl create deployment --image redis redis 
kubectl expose deployment my-nginx --port=80 --type=LoadBalancer

npm run compile && cdk8s synth 
